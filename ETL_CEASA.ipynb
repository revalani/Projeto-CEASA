{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto CEASA\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Facilitar o acesso aos dados do Centrais de Abastecimento do Estado do Rio de Janeiro S.A (CEASA-RJ) por meio de um processo automatizado de coleta, transformação e análise de dados.\n",
    "\n",
    "## Motivação\n",
    "\n",
    "A necessidade de disponibilizar informações do CEASA-RJ de maneira mais acessível, transformando dados presentes em PDFs em formatos utilizáveis para análise e insights mais rápidos.\n",
    "\n",
    "## Etapas do Projeto\n",
    "1. Coleta de Dados\n",
    "2. Transformação de Dados\n",
    "3. Armazenamento\n",
    "4. Análise e Visualização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. configurações inicia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/bin/activate\n",
    "%pip install -r minimal.requirements.txt # type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # carregando variaveis do embiente \n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv(\".env\")\n",
    "# ceasa_lista_pdf\n",
    "\n",
    "# variaeis de embiente\n",
    "\n",
    "URL_CEASA   = 'https://www.ceasa.rj.gov.br'\n",
    "URL_CEASA_cotacao   = URL_CEASA + '/Cota%C3%A7%C3%A3o'\n",
    "\n",
    "PASTA_DADOS = \"./dados/\"\n",
    "PASTA_PDFs  = \"./dados/pdfs/\"\n",
    "\n",
    "ceasa_lista_pdf = PASTA_DADOS + \"ceasa_lista_pdf.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Coletar Dados\n",
    "\n",
    "consiste nas etapas:\n",
    "1. Encontar as urls dos documentos pdf\n",
    "2. com a url do documento, extrair nome e data do documento\n",
    "3. Baixar e armazenar url, nome do documento e data em uma base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coleta linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re #regex\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Configurando o pool HTTP do urllib3\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "def foi_extraida(url):\n",
    "\t\"\"\"\n",
    "\tverifica se uma url ja está contida na base de dados.\n",
    "\t\"\"\"\n",
    "\treturn True if url in((df_ceasa_lista_pdf['URL'].eq(url))) else False\n",
    "\n",
    "def pegar_links(url,lista_urls):\n",
    "\tresponse = http.request('GET', url)\n",
    "\tsoup = BeautifulSoup(response.data, 'html.parser')\n",
    "\n",
    "\tfor link in soup.select('#main a'):\n",
    "\t\ta = str(link.get('href'))\n",
    "\t\tif \".pdf\" in a:\n",
    "\t\t\ta = URL_CEASA + a\n",
    "\t\t\tif a in urls_na_base:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tlista_urls.append(a)\n",
    "\t\telse:\n",
    "\t\t\tpegar_links(a,lista_urls)\n",
    "\n",
    "def extrair_dados_url(url):\n",
    "\t# pega nome do arquivo na URL\n",
    "\tnome_arquivo = url.split(\"/\")[-1]\n",
    "\tnome_arquivo = requests.utils.unquote(nome_arquivo) # type: ignore\n",
    "\n",
    "\t# padrao de dd mm yyyy para data\n",
    "\tmatches = re.findall(r'(\\d{2})\\s(\\d{2})\\s(\\d{4})', nome_arquivo)\n",
    "\tdata = dt.strptime(\"/\".join(matches[0]), \"%d/%m/%Y\")\n",
    "\n",
    "\treturn url,\"CEASA-RJ_\" + nome_arquivo.replace(\" \", \"_\") ,data\n",
    "\n",
    "def pegar_pdf(url,nome_arquivo):\n",
    "\t\"\"\"\n",
    "    Baixa um arquivo PDF a partir de uma URL e o armazena localmente.\n",
    "\n",
    "    Args:\n",
    "    - url (str): URL do arquivo PDF.\n",
    "    - nome_arquivo (str): Nome do arquivo a ser salvo.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True se o arquivo foi baixado com sucesso, False caso contrário.\n",
    "    \"\"\"\n",
    "\n",
    "\tif not os.path.exists(PASTA_PDFs + nome_arquivo):\n",
    "\t\tresponse = requests.get(url)\n",
    "\n",
    "\t\tif response.status_code == 200:\n",
    "\t\t\twith open(PASTA_PDFs + nome_arquivo, \"wb\") as f:\n",
    "\t\t\t\tf.write(response.content)\n",
    "\t\t\t\n",
    "\t\t\treturn True\t\n",
    "\n",
    "def atualizar_csv(url, nome_arquivo,data):\n",
    "\ttmp_file = \"tmp_url_nome_data.parquet\"\n",
    "\n",
    "\tif os.path.exists(tmp_file):\n",
    "        # Se o arquivo existir, carregue o DataFrame existente\n",
    "\t\tdf = pd.read_parquet(tmp_file)\n",
    "\telse:\n",
    "        # Se o arquivo não existir, crie um novo DataFrame\n",
    "\t\tdf = pd.DataFrame(columns=['URL', 'NOME_ARQUIVO', 'DATA'])\n",
    "\t\tdf = df.astype({'URL': 'string', 'NOME_ARQUIVO': 'string','DATA': 'datetime64[ns]'})\n",
    "\n",
    "\tdata_to_file = {\n",
    "\t\t'URL': [url],\n",
    "\t\t'NOME_ARQUIVO': [nome_arquivo],\n",
    "\t\t'DATA': [data]\n",
    "\t} \n",
    "\t# df = pd.DataFrame(data_to_file)\n",
    "\t# df.to_parquet(tmp_file, index=False, engine='fastparquet', append=True)\n",
    "\n",
    "\t# data_to_file = {\n",
    "\t# \t'URL': [url],\n",
    "\t# \t'NOME_ARQUIVO': [nome_arquivo],\n",
    "\t# \t'DATA': [data]\n",
    "\t# } \n",
    "\tdf_novalinha = pd.DataFrame(data_to_file)\n",
    "\t# df.to_parquet(tmp_file, index=False, engine='fastparquet', append=True)\n",
    "\n",
    "\n",
    "\tdf = pd.concat([df, df_novalinha], ignore_index=True)\n",
    "    \n",
    "\tdf.to_parquet(tmp_file, index=False, engine='fastparquet')\n",
    "\n",
    "\n",
    "\t\n",
    "\t# if os.path.exists(tmp_file):\n",
    "    #     # Se o arquivo existir, carregue o DataFrame existente\n",
    "\t# \tdf = pd.read_parquet(tmp_file)\n",
    "\t# else:\n",
    "    #     # Se o arquivo não existir, crie um novo DataFrame\n",
    "\n",
    "\t# data_to_file = {\n",
    "\t# \t'URL': [url],\n",
    "\t# \t'NOME_ARQUIVO': [nome_arquivo],\n",
    "\t# \t'DATA': [data]\n",
    "\t# } \n",
    "\t# df = pd.DataFrame(data_to_file)\n",
    "\t# df.to_parquet(tmp_file, index=False, engine='fastparquet', append=True)\n",
    "\n",
    "\t# data_to_file = {\n",
    "\t# \t'URL': [url],\n",
    "\t# \t'NOME_ARQUIVO': [nome_arquivo],\n",
    "\t# \t'DATA': [data]\n",
    "\t# } \n",
    "\t# df = pd.DataFrame(columns=['URL', 'NOME_ARQUIVO', 'DATA'])\n",
    "\t# df = df.astype({'URL': 'string', 'NOME_ARQUIVO': 'string','DATA': 'datetime64[ns]'})\n",
    "\t# # df_novalinha = pd.DataFrame(data_to_file)\n",
    "\t# df.to_parquet(tmp_file, index=False, engine='fastparquet', append=True)\n",
    "\n",
    "\n",
    "\t# # df = pd.concat([df, df_novalinha], ignore_index=True)\n",
    "    \n",
    "\t# # df.to_parquet(tmp_file, index=False, engine='fastparquet', mode='overwrite')\n",
    "\t\n",
    "def filtro_data_modelo(data):\n",
    "\treturn data > dt(2023, 3, 1)\n",
    "\n",
    "def atualizar_base_pdf():\n",
    "\t# atualiza as urls de novos PDFf\n",
    "\n",
    "\turls = []\n",
    "\tpegar_links(URL_CEASA_cotacao,urls)\n",
    "\n",
    "\t# remover URL >março/23\n",
    "\t# WIP: tretamento pdfs de modelos antes de 01-03-2023 \n",
    "\t\n",
    "\t\n",
    "\t# Coleta pdf, armazena o arquivo e atualiza a base \n",
    "\tfor index, url in enumerate(urls):\n",
    "\t\tprint(f\"{index} de {len(urls)}\",end=\"\\t\\t\\r\")\n",
    "\n",
    "\t\turl, nome_arquivo, data = extrair_dados_url(url)\n",
    "\n",
    "\t\tif filtro_data_modelo(data): # filtro \n",
    "\t\t\t# urls[index] = [url, nome_arquivo, data]\n",
    "\t\t\t# pegar_pdf(url,nome_arquivo)\n",
    "\t\t\tatualizar_csv(url,nome_arquivo,data)\n",
    "\t\t# else:\n",
    "\t\t# \turls.pop(index)\n",
    "\t\t\n",
    "\t\tprint(data)\n",
    "\t# urls = filter(lambda x: x[1] != \"\", urls) \n",
    "\turls = [i for i in urls if (i[1] != \"\" )]\n",
    "\n",
    "\t# return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executar extração de pdf\n",
    "\n",
    "observação:\n",
    "mudar de biblioteca requeste para urllib3 e colocar coleta paralela, saiu de 15s para 1,7s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-24 00:00:00\n",
      "2023-11-23 00:00:00\n",
      "2023-11-22 00:00:00\n",
      "2023-11-21 00:00:00\n",
      "2023-11-17 00:00:00\n",
      "2023-11-16 00:00:00\n",
      "2023-11-14 00:00:00\n",
      "2023-07-31 00:00:00\n",
      "2023-07-28 00:00:00\n",
      "2023-07-27 00:00:00\n",
      "2023-07-26 00:00:00\n",
      "2023-07-25 00:00:00\n",
      "2023-07-24 00:00:00\n",
      "2023-07-21 00:00:00\n",
      "2023-07-20 00:00:00\n",
      "2023-07-19 00:00:00\n",
      "2023-07-18 00:00:00\n",
      "2023-07-17 00:00:00\n",
      "2023-07-14 00:00:00\n",
      "2023-07-13 00:00:00\n",
      "2023-07-12 00:00:00\n",
      "2023-07-11 00:00:00\n",
      "2023-07-10 00:00:00\n",
      "2023-07-07 00:00:00\n",
      "2023-07-06 00:00:00\n",
      "2023-07-05 00:00:00\n",
      "2023-07-04 00:00:00\n",
      "2023-07-03 00:00:00\n",
      "2023-06-30 00:00:00\n",
      "2023-06-29 00:00:00\n",
      "2023-06-28 00:00:00\n",
      "2023-06-27 00:00:00\n",
      "2023-06-26 00:00:00\n",
      "2023-06-23 00:00:00\n",
      "2023-06-22 00:00:00\n",
      "2023-06-21 00:00:00\n",
      "2023-06-20 00:00:00\n",
      "2023-06-19 00:00:00\n",
      "2023-06-16 00:00:00\n",
      "2023-06-15 00:00:00\n",
      "2023-06-14 00:00:00\n",
      "2023-06-13 00:00:00\n",
      "2023-06-12 00:00:00\n",
      "2023-06-07 00:00:00\n",
      "2023-06-06 00:00:00\n",
      "2023-06-05 00:00:00\n",
      "2023-06-02 00:00:00\n",
      "2023-06-01 00:00:00\n",
      "2023-05-31 00:00:00\n",
      "2023-05-30 00:00:00\n",
      "2023-05-29 00:00:00\n",
      "2023-05-26 00:00:00\n",
      "2023-05-25 00:00:00\n",
      "2023-05-24 00:00:00\n",
      "2023-05-23 00:00:00\n",
      "2023-05-22 00:00:00\n",
      "2023-05-19 00:00:00\n",
      "2023-05-18 00:00:00\n",
      "2023-05-17 00:00:00\n",
      "2023-05-16 00:00:00\n",
      "2023-05-15 00:00:00\n",
      "2023-05-12 00:00:00\n",
      "2023-05-11 00:00:00\n",
      "2023-05-10 00:00:00\n",
      "2023-05-09 00:00:00\n",
      "2023-05-08 00:00:00\n",
      "2023-05-05 00:00:00\n",
      "2023-05-04 00:00:00\n",
      "2023-05-03 00:00:00\n",
      "2023-05-02 00:00:00\n",
      "2023-04-28 00:00:00\n",
      "2023-04-27 00:00:00\n",
      "2023-04-26 00:00:00\n",
      "2023-04-25 00:00:00\n",
      "2023-04-24 00:00:00\n",
      "2023-04-20 00:00:00\n",
      "2023-04-19 00:00:00\n",
      "2023-04-18 00:00:00\n",
      "2023-04-17 00:00:00\n",
      "2023-04-14 00:00:00\n",
      "2023-04-13 00:00:00\n",
      "2023-04-12 00:00:00\n",
      "2023-04-11 00:00:00\n",
      "2023-04-10 00:00:00\n",
      "2023-04-05 00:00:00\n",
      "2023-04-04 00:00:00\n",
      "2023-04-03 00:00:00\n",
      "2023-03-31 00:00:00\n",
      "2023-03-30 00:00:00\n",
      "2023-03-29 00:00:00\n",
      "2023-03-28 00:00:00\n",
      "2023-03-27 00:00:00\n",
      "2023-03-24 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-03-22 00:00:00\n",
      "2023-03-21 00:00:00\n",
      "2023-03-20 00:00:00\n",
      "2023-03-17 00:00:00\n",
      "2023-03-16 00:00:00\n",
      "2023-03-15 00:00:00\n",
      "2023-03-14 00:00:00\n",
      "2023-03-13 00:00:00\n",
      "2023-03-10 00:00:00\n",
      "2023-03-09 00:00:00\n",
      "2023-03-08 00:00:00\n",
      "2023-03-07 00:00:00\n",
      "2023-03-06 00:00:00\n",
      "2023-03-03 00:00:00\n",
      "2023-03-02 00:00:00\n",
      "2023-03-01 00:00:00\n",
      "2023-02-28 00:00:00\n",
      "2023-02-27 00:00:00\n",
      "2023-02-24 00:00:00\n",
      "2023-02-23 00:00:00\n",
      "2023-02-16 00:00:00\n",
      "2023-02-15 00:00:00\n",
      "2023-02-14 00:00:00\n",
      "2023-02-13 00:00:00\n",
      "2023-02-10 00:00:00\n",
      "2023-02-09 00:00:00\n",
      "2023-02-08 00:00:00\n",
      "2023-02-07 00:00:00\n",
      "2023-02-06 00:00:00\n",
      "2023-02-03 00:00:00\n",
      "2023-02-02 00:00:00\n",
      "2023-02-01 00:00:00\n",
      "2023-01-31 00:00:00\n",
      "2023-01-30 00:00:00\n",
      "2023-01-27 00:00:00\n",
      "2023-01-26 00:00:00\n",
      "2023-01-25 00:00:00\n",
      "2023-01-24 00:00:00\n",
      "2023-01-23 00:00:00\n",
      "2023-01-19 00:00:00\n",
      "2023-01-18 00:00:00\n",
      "2023-01-17 00:00:00\n",
      "2023-01-16 00:00:00\n",
      "2023-01-13 00:00:00\n",
      "2023-01-12 00:00:00\n",
      "2023-01-11 00:00:00\n",
      "2023-01-10 00:00:00\n",
      "2023-01-09 00:00:00\n",
      "2023-01-06 00:00:00\n",
      "2023-01-05 00:00:00\n",
      "2023-01-04 00:00:00\n",
      "2023-01-03 00:00:00\n",
      "2023-01-02 00:00:00\n",
      "2022-12-30 00:00:00\n",
      "2022-12-29 00:00:00\n",
      "2022-12-28 00:00:00\n",
      "2022-12-27 00:00:00\n",
      "2022-12-26 00:00:00\n",
      "2022-12-23 00:00:00\n",
      "2022-12-22 00:00:00\n",
      "2022-12-21 00:00:00\n",
      "2022-12-20 00:00:00\n",
      "2022-12-19 00:00:00\n",
      "2022-12-16 00:00:00\n",
      "2022-12-15 00:00:00\n",
      "2022-12-14 00:00:00\n",
      "2022-12-13 00:00:00\n",
      "2022-12-12 00:00:00\n",
      "2022-12-09 00:00:00\n",
      "2022-12-08 00:00:00\n",
      "2022-12-07 00:00:00\n",
      "2022-12-06 00:00:00\n",
      "2022-12-05 00:00:00\n",
      "2022-12-02 00:00:00\n",
      "2022-12-01 00:00:00\n",
      "2022-11-30 00:00:00\n",
      "2022-11-29 00:00:00\n",
      "2022-11-25 00:00:00\n",
      "2022-11-24 00:00:00\n",
      "2022-11-23 00:00:00\n",
      "2022-11-22 00:00:00\n",
      "2022-11-21 00:00:00\n",
      "2022-11-18 00:00:00\n",
      "2022-11-17 00:00:00\n",
      "2022-11-16 00:00:00\n",
      "2022-11-11 00:00:00\n",
      "2022-11-10 00:00:00\n",
      "2022-11-09 00:00:00\n",
      "2022-11-08 00:00:00\n",
      "2022-11-07 00:00:00\n",
      "2022-11-04 00:00:00\n",
      "2022-11-03 00:00:00\n",
      "2022-11-01 00:00:00\n",
      "2022-10-31 00:00:00\n",
      "2022-10-28 00:00:00\n",
      "2022-10-27 00:00:00\n",
      "2022-10-26 00:00:00\n",
      "2022-10-25 00:00:00\n",
      "2022-10-24 00:00:00\n",
      "2022-10-21 00:00:00\n",
      "2022-10-20 00:00:00\n",
      "2022-10-19 00:00:00\n",
      "2022-10-18 00:00:00\n",
      "2022-10-14 00:00:00\n",
      "2022-10-13 00:00:00\n",
      "2022-10-11 00:00:00\n",
      "2022-10-10 00:00:00\n",
      "2022-10-07 00:00:00\n",
      "2022-10-06 00:00:00\n",
      "2022-10-04 00:00:00\n",
      "2022-10-03 00:00:00\n",
      "2022-09-29 00:00:00\n",
      "2022-09-28 00:00:00\n",
      "2022-09-27 00:00:00\n",
      "2022-09-26 00:00:00\n",
      "2022-09-23 00:00:00\n",
      "2022-09-22 00:00:00\n",
      "2022-09-21 00:00:00\n",
      "2022-09-20 00:00:00\n",
      "2022-09-19 00:00:00\n",
      "2022-09-16 00:00:00\n",
      "2022-09-15 00:00:00\n",
      "2022-09-14 00:00:00\n",
      "2022-09-13 00:00:00\n",
      "2022-09-12 00:00:00\n",
      "2022-09-09 00:00:00\n",
      "2022-09-08 00:00:00\n",
      "2022-09-06 00:00:00\n",
      "2022-09-05 00:00:00\n",
      "2022-09-02 00:00:00\n",
      "2022-09-01 00:00:00\n",
      "2022-08-26 00:00:00\n",
      "2022-08-25 00:00:00\n",
      "2022-08-24 00:00:00\n",
      "2022-08-23 00:00:00\n",
      "2022-08-22 00:00:00\n",
      "2022-08-19 00:00:00\n",
      "2022-08-18 00:00:00\n",
      "2022-08-17 00:00:00\n",
      "2022-08-16 00:00:00\n",
      "2022-08-15 00:00:00\n",
      "2022-08-12 00:00:00\n",
      "2022-08-11 00:00:00\n",
      "2022-08-10 00:00:00\n",
      "2022-08-09 00:00:00\n",
      "2022-08-08 00:00:00\n",
      "2022-08-05 00:00:00\n",
      "2022-08-04 00:00:00\n",
      "2022-08-03 00:00:00\n",
      "2022-08-02 00:00:00\n",
      "2022-08-01 00:00:00\n",
      "2022-07-28 00:00:00\n",
      "2022-07-27 00:00:00\n",
      "2022-07-26 00:00:00\n",
      "2022-07-25 00:00:00\n",
      "2022-07-22 00:00:00\n",
      "2022-07-21 00:00:00\n",
      "2022-07-20 00:00:00\n",
      "2022-07-19 00:00:00\n",
      "2022-07-18 00:00:00\n",
      "2022-07-15 00:00:00\n",
      "2022-07-14 00:00:00\n",
      "2022-07-13 00:00:00\n",
      "2022-07-12 00:00:00\n",
      "2022-07-11 00:00:00\n",
      "2022-07-08 00:00:00\n",
      "2022-07-07 00:00:00\n",
      "2022-07-06 00:00:00\n",
      "2022-07-05 00:00:00\n",
      "2022-07-04 00:00:00\n",
      "2022-07-01 00:00:00\n",
      "2022-06-30 00:00:00\n",
      "2022-06-29 00:00:00\n",
      "2022-06-28 00:00:00\n",
      "2022-06-27 00:00:00\n",
      "2022-06-24 00:00:00\n",
      "2022-06-23 00:00:00\n",
      "2022-06-22 00:00:00\n",
      "2022-06-21 00:00:00\n",
      "2022-06-20 00:00:00\n",
      "2022-06-15 00:00:00\n",
      "2022-06-14 00:00:00\n",
      "2022-06-13 00:00:00\n",
      "2022-06-10 00:00:00\n",
      "2022-06-09 00:00:00\n",
      "2022-06-08 00:00:00\n",
      "2022-06-07 00:00:00\n",
      "2022-06-06 00:00:00\n",
      "2022-06-03 00:00:00\n",
      "2022-06-02 00:00:00\n",
      "2022-06-01 00:00:00\n",
      "2022-05-27 00:00:00\n",
      "2022-05-26 00:00:00\n",
      "2022-05-25 00:00:00\n",
      "2022-05-24 00:00:00\n",
      "2022-05-23 00:00:00\n",
      "2022-05-20 00:00:00\n",
      "2022-05-19 00:00:00\n",
      "2022-05-18 00:00:00\n",
      "2022-05-17 00:00:00\n",
      "2022-05-16 00:00:00\n",
      "2022-05-13 00:00:00\n",
      "2022-05-12 00:00:00\n",
      "2022-05-11 00:00:00\n",
      "2022-05-10 00:00:00\n",
      "2022-05-09 00:00:00\n",
      "2022-05-06 00:00:00\n",
      "2022-05-05 00:00:00\n",
      "2022-05-04 00:00:00\n",
      "2022-05-03 00:00:00\n",
      "2022-05-02 00:00:00\n",
      "2022-04-29 00:00:00\n",
      "2022-04-28 00:00:00\n",
      "2022-04-27 00:00:00\n",
      "2022-04-26 00:00:00\n",
      "2022-04-25 00:00:00\n",
      "2022-04-19 00:00:00\n",
      "2022-04-18 00:00:00\n",
      "2022-04-13 00:00:00\n",
      "2022-04-12 00:00:00\n",
      "2022-04-11 00:00:00\n",
      "2022-04-08 00:00:00\n",
      "2022-04-07 00:00:00\n",
      "2022-04-06 00:00:00\n",
      "2022-04-05 00:00:00\n",
      "2022-04-04 00:00:00\n",
      "2022-04-01 00:00:00\n",
      "2022-03-30 00:00:00\n",
      "2022-03-29 00:00:00\n",
      "2022-03-28 00:00:00\n",
      "2022-03-25 00:00:00\n",
      "2022-03-24 00:00:00\n",
      "2022-03-23 00:00:00\n",
      "2022-03-22 00:00:00\n",
      "2022-03-21 00:00:00\n",
      "2022-03-18 00:00:00\n",
      "2022-03-17 00:00:00\n",
      "2022-03-16 00:00:00\n",
      "2022-03-15 00:00:00\n",
      "2022-03-14 00:00:00\n",
      "2022-03-11 00:00:00\n",
      "2022-03-10 00:00:00\n",
      "2022-03-09 00:00:00\n",
      "2022-03-08 00:00:00\n",
      "2022-03-07 00:00:00\n",
      "2022-03-04 00:00:00\n",
      "2022-03-03 00:00:00\n",
      "2022-02-25 00:00:00\n",
      "2022-02-24 00:00:00\n",
      "2022-02-23 00:00:00\n",
      "2022-02-22 00:00:00\n",
      "2022-02-21 00:00:00\n",
      "2022-02-18 00:00:00\n",
      "2022-02-17 00:00:00\n",
      "2022-02-16 00:00:00\n",
      "2022-02-15 00:00:00\n",
      "2022-02-14 00:00:00\n",
      "2022-02-11 00:00:00\n",
      "2022-02-10 00:00:00\n",
      "2022-02-09 00:00:00\n",
      "2022-02-08 00:00:00\n",
      "2022-02-07 00:00:00\n",
      "2022-02-04 00:00:00\n",
      "2022-02-03 00:00:00\n",
      "2022-02-02 00:00:00\n",
      "2022-02-01 00:00:00\n",
      "2022-01-31 00:00:00\n",
      "2022-01-28 00:00:00\n",
      "2022-01-27 00:00:00\n",
      "2022-01-26 00:00:00\n",
      "2022-01-25 00:00:00\n",
      "2022-01-24 00:00:00\n",
      "2022-01-19 00:00:00\n",
      "2022-01-18 00:00:00\n",
      "2022-01-17 00:00:00\n",
      "2022-01-14 00:00:00\n",
      "2022-01-13 00:00:00\n",
      "2022-01-12 00:00:00\n",
      "2022-01-11 00:00:00\n",
      "2022-01-10 00:00:00\n",
      "2022-01-07 00:00:00\n",
      "2022-01-06 00:00:00\n",
      "2022-01-05 00:00:00\n",
      "2022-01-04 00:00:00\n",
      "2022-01-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ceasa_lista_pdf = PASTA_DADOS + \"ceasa_lista_pdf.parquet\"\n",
    "ceasa_lista_pdf = PASTA_DADOS + \"dados_pdfs_v2.parquet\"\n",
    "\n",
    "df = pd.read_parquet(ceasa_lista_pdf)\n",
    "urls_na_base = df['URL'].drop_duplicates().to_list()\n",
    "\n",
    "atualizar_base_pdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformando Dados\n",
    "\n",
    "A parte mais deliada é garantir a consistencia dos dados extraidos dos documentos pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compressão paralelizada (muito custoso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import concurrent.futures\n",
    "\n",
    "def compress_pdf(input_path, output_path):\n",
    "    with open(input_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page.compress_content_streams()\n",
    "            pdf_writer.add_page(page)\n",
    "\n",
    "        with open(output_path, 'wb') as output_file:\n",
    "            pdf_writer.write(output_file)\n",
    "\n",
    "def process_pdf(file_name):\n",
    "    print(f'arquivo atual: {file_name}')\n",
    "    compress_pdf(PASTA_PDFs + file_name, PASTA_PDFs + file_name)\n",
    "\n",
    "# Sua lista de arquivos\n",
    "files_to_process = df_extração[\"nome_arquivo\"].tolist()\n",
    "\n",
    "# Número máximo de threads (ajuste conforme necessário)\n",
    "max_threads = 4\n",
    "\n",
    "# Usando ThreadPoolExecutor para paralelizar\n",
    "with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "    futures = [executor.submit(process_pdf, file_name) for file_name in files_to_process]\n",
    "\n",
    "    # Esperar que todas as threads concluam\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        count += 1 # type: ignore\n",
    "        print_percent_done(count, len(files_to_process), future.result()) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo Tabelas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apartir de 01-03-2023 ouver mudança na formatação da tabela.\n",
    "trataremos as duas versões de PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "def extrair_tabela_pdf(pdf_path, pagina=0):\n",
    "    # Abre o PDF com pdfplumber\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Seleciona a página desejada\n",
    "        page = pdf.pages[pagina]\n",
    "        \n",
    "        # Extrai a tabela como uma lista de dicionários\n",
    "        table_data = page.extract_table()\n",
    "\n",
    "        # Converte a lista de dicionários para um DataFrame do pandas\n",
    "        df = pd.DataFrame(table_data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fazendo tratamento na tabela 1 do pdf modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabeçalho\n",
    "header = ['PRODUTOS', 'TIPO', 'UNIDADE EMBALAGEM', 'VARIAÇÃO ULTIMOS 12 MESES', 'MIN', 'MODAL', 'MAX']\n",
    "\n",
    "#c Classes de alimentos\n",
    "classes = ['1. FRUTAS NACIONAIS',\n",
    "          '2. FRUTAS IMPORTADAS',\n",
    "          '3. HORTALIÇAS FRUTO',\n",
    "          '4. HORTALIÇAS FOLHA, FLOR E HASTE',\n",
    "          '5. HORTALIÇAS RAIZ, BULBO,TUBÉRCULO E RIZOMA',\n",
    "          '6. OVOS',\n",
    "          '7. PEIXE']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tramento PDF v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# tratamendo geral pdf v2\n",
    "def tratamento_1_tbv2(df):\n",
    "    # define cabeçalhos\n",
    "    df.columns = header\n",
    "    \n",
    "    # Remove primeiras linhas\n",
    "    df = df.iloc[2: , :]\n",
    "    \n",
    "    # Adiciona colula classe\n",
    "    df.loc[:, 'CLASSE'] = ''\n",
    "\n",
    "    return df\n",
    "\n",
    "def tratamento_2_tbv2(df, arquivo='', data_pdf = '', url='' ):\n",
    "    # preencher class\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    try:\n",
    "        # Colulas de arquivo e data\n",
    "        df['DATA'] = data_pdf\n",
    "        df['ARQUIVO'] = arquivo\n",
    "        df['URL'] = url\n",
    "        \n",
    "        # preenche classe\n",
    "        def preenche_classe(df):\n",
    "            df = df.reset_index(drop=True)    \n",
    "            \n",
    "            current_class = ''  # Para rastrear a classe atual\n",
    "            for index, row in df.iterrows():\n",
    "                if row['PRODUTOS'] in classes:\n",
    "                    current_class = row['PRODUTOS']\n",
    "                df.at[index,'CLASSE'] = current_class\n",
    "        \n",
    "            df = df[df['PRODUTOS'] != df['CLASSE']]\n",
    "            df = df.reset_index(drop=True)    \n",
    "        \n",
    "            return df\n",
    "        \n",
    "        df = preenche_classe(df)\n",
    "\n",
    "        \n",
    "        # Replace all instances of \",\" with \".\" in columns: 'MIN', 'MODAL', 'MAX'\n",
    "        df['MIN'] = df['MIN'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "        df['MODAL'] = df['MODAL'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "        df['MAX'] = df['MAX'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "\n",
    "        # Replace all instances of \"\" with \"0\" in columns: 'MIN', 'MODAL', 'MAX'\n",
    "        df.loc[df['MIN'].str.lower() == \"\".lower(), 'MIN'] = \"0\"\n",
    "        df.loc[df['MODAL'].str.lower() == \"\".lower(), 'MODAL'] = \"0\"\n",
    "        df.loc[df['MAX'].str.lower() == \"\".lower(), 'MAX'] = \"0\"\n",
    "\n",
    "        # Função para limpar os valores da coluna\n",
    "        def limpar_valor(valor):\n",
    "            # Remover símbolos duplicados\n",
    "            valor = ''.join(ch for ch, _ in zip(valor, valor[1:] + '\\0') if ch != _)\n",
    "            # Remover espaços em branco duplicados\n",
    "            valor = ' '.join(valor.split())\n",
    "            return valor\n",
    "\n",
    "        # Aplicar a função de limpeza à coluna\n",
    "        df['MIN'] = df['MIN'].apply(limpar_valor)\n",
    "        df['MAX'] = df['MAX'].apply(limpar_valor)\n",
    "        df['MODAL'] = df['MODAL'].apply(limpar_valor)\n",
    "\n",
    "        # df = df.applymap(lambda x: limpar_valor(x))  # Remove espaços no início e no final\n",
    "        df = df.map(lambda x: str(x).strip())  # Remove espaços no início e no final\n",
    "        df = df.map(lambda x: ' '.join(str(x).split()))  # Remove espaços duplicados\n",
    "\n",
    "\n",
    "        # Change column type to float32 for columns: 'MIN', 'MODAL', 'MAX'\n",
    "        df = df.astype({'MIN': 'float32', 'MODAL': 'float32', 'MAX': 'float32'})\n",
    "\n",
    "        # Replace all instances of \"S/C\" with \"0\" in column: 'VARIAÇÃO ULTIMOS 12 MESES'\n",
    "        df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace(\"S/C\", \"0\", case=False, regex=False)\n",
    "\n",
    "        # Replace all instances of \"\" with \"\" in column: 'VARIAÇÃO ULTIMOS 12 MESES'\n",
    "        df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace(\"\", \"\", case=False, regex=False)\n",
    "\n",
    "        df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace('%', '')  # Remover o símbolo de percentagem\n",
    "        df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace(',', '.')  # Remover o símbolo de percentagem\n",
    "\n",
    "        df['VARIAÇÃO ULTIMOS 12 MESES'] = pd.to_numeric(df['VARIAÇÃO ULTIMOS 12 MESES'], errors='coerce') / 100  # Converter para float e dividir por 100\n",
    "        df = df.astype({'VARIAÇÃO ULTIMOS 12 MESES': 'float32'})\n",
    "\n",
    "\n",
    "        # Convert text to uppercase in columns: 'PRODUTOS', 'TIPO'\n",
    "        df['PRODUTOS'] = df['PRODUTOS'].str.upper()\n",
    "        df['TIPO'] = df['TIPO'].str.upper()\n",
    "\n",
    "        # Change column type to string for columns: 'PRODUTOS', 'TIPO'\n",
    "        df = df.astype({'PRODUTOS': 'string', 'TIPO': 'string','UNIDADE EMBALAGEM': 'string'})\n",
    "\n",
    "        # Convert text to uppercase in column: 'UNIDADE EMBALAGEM'\n",
    "        df['UNIDADE EMBALAGEM'] = df['UNIDADE EMBALAGEM'].str.upper()\n",
    "\n",
    "        # Replace all instances of \",\" with \".\" in column: 'UNIDADE EMBALAGEM'\n",
    "        df['UNIDADE EMBALAGEM'] = df['UNIDADE EMBALAGEM'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "\n",
    "        # Round columns 'VARIAÇÃO ULTIMOS 12 MESES', 'MIN' and 2 other columns (Number of decimals: 4)\n",
    "        df = df.round({'VARIAÇÃO ULTIMOS 12 MESES': 4, 'MIN': 2, 'MODAL': 2, 'MAX': 2})\n",
    "\n",
    "        # Change column type to datetime64[ns] for column: 'DATA\n",
    "        df = df.astype({'CLASSE': 'string', 'ARQUIVO': 'string', 'URL': 'string','DATA': 'datetime64[ns]'})\n",
    "\n",
    "        # Sort by column: 'DATA' (descending)\n",
    "        df = df.sort_values(['DATA'], ascending=[False])\n",
    "\n",
    "        df = df.reset_index(drop=True)\n",
    "    except  (ValueError) as e:\n",
    "        pass\n",
    "        # # Lidar com o erro (substituir por string vazia)\n",
    "        # print(f\"Erro ao converter a coluna: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pdf_v2_para_tabela(pdf_v2, data=None, url=''):        \n",
    "    # Compreensão de lista para criar a lista de dataframes\n",
    "    list_df_tratamento_1_tbv2 = [ tratamento_1_tbv2( extrair_tabela_pdf(PASTA_PDFs + pdf_v2, i)) for i in range(5) ]\n",
    "\n",
    "    # Concatenar os dataframes da lista\n",
    "    df = pd.concat(list_df_tratamento_1_tbv2.copy())\n",
    "    return tratamento_2_tbv2(df,pdf_v2,data,url) # type: ignore\n",
    "\n",
    "\n",
    "df = pdf_v2_para_tabela('CEASA-RJ_Boletim_diário_de_preços__31_07_2023.pdf')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import os\n",
    "\n",
    "\n",
    "# Suprime os avisos SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "\n",
    "def extrair_tb_pdf_V2():\n",
    "    # comparar com\n",
    "    df_pdfs_v2 = filtar_para_novos_pdf()\n",
    "    # df_pdfs_v2 = pd.read_parquet(ceasa_lista_pdf)\n",
    "    \n",
    "    df_pdfs_v2 = df_pdfs_v2[df_pdfs_v2['DATA']  >= '2023-03-01']\n",
    "    df_pdfs_v2 = df_pdfs_v2[:]\n",
    "    \n",
    "    len = df_pdfs_v2.shape[0]\n",
    "    lista_pdf_v2 = []\n",
    "    try:\n",
    "        for index, row in df_pdfs_v2.iterrows():\n",
    "            arquivo = row['NOME_ARQUIVO']\n",
    "\n",
    "            data = row['DATA']\n",
    "            url = row['URL']\n",
    "            \n",
    "            print(f\"{index} de {len}\", data, end=\"\\t\\t\\r\")\n",
    "\n",
    "            df_tmp = pdf_v2_para_tabela(\n",
    "                arquivo, data, url)\n",
    "\n",
    "            lista_pdf_v2.append(df_tmp)\n",
    "    except:\n",
    "        print(\"erro\")\n",
    "\n",
    "    finally:\n",
    "        df = pd.concat(lista_pdf_v2)\n",
    "        return df\n",
    "\n",
    "def filtar_para_novos_pdf():\n",
    "\n",
    "    df_pdfs_v2 = pd.read_parquet(ceasa_lista_pdf)\n",
    "    df_dados_pdfs_v2 = PASTA_DADOS + \"dados_pdfs_v2.parquet\"\n",
    "    \n",
    "    if os.path.exists(df_dados_pdfs_v2):\n",
    "\n",
    "        df_pdfs_v2 = pd.read_parquet(ceasa_lista_pdf)\n",
    "\n",
    "        df_dados_pdfs_v2 = pd.read_parquet(PASTA_DADOS + \"dados_pdfs_v2.parquet\")\n",
    "        df_dados_pdfs_v2 = df_dados_pdfs_v2.groupby(['DATA']).agg(C=('DATA', 'first')).reset_index()\n",
    "\n",
    "        df_pdfs_v2 = df_pdfs_v2[~df_pdfs_v2['DATA'].isin(df_dados_pdfs_v2['DATA'])]\n",
    "\n",
    "        # df_dados_pdfs_v2 = pd.read_parquet(df_dados_pdfs_v2)\n",
    "        # # df_dados_pdfs_v2 = df_dados_pdfs_v2[\"URL\"].drop_duplicates()\n",
    "        # print(df_pdfs_v2.shape[0],\" / \",df_dados_pdfs_v2.shape[0])\n",
    "\n",
    "        # df_pdfs_v2=pd.merge(df_pdfs_v2,df_dados_pdfs_v2,on='DATA', how='left')\n",
    "\n",
    "        # # df_pdfs_v2 = df_pdfs_v2[~df_pdfs_v2['URL'].isin(df_dados_pdfs_v2)]\n",
    "\n",
    "    return df_pdfs_v2\n",
    "\n",
    "\n",
    "df = extrair_tb_pdf_V2()\n",
    "\n",
    "try:\n",
    "    # df.to_csv(PASTA_DADOS + 'dados_pdfs_v2.csv',index=False, mode='a')\n",
    "    df.to_parquet(PASTA_DADOS + 'dados_pdfs_v2.parquet',index=False,engine='fastparquet', append=True)\n",
    "except:\n",
    "    # df.to_csv(PASTA_DADOS + 'dados_pdfs_v2.csv',index=False)\n",
    "    df.to_parquet(PASTA_DADOS + 'dados_pdfs_v2.parquet',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
