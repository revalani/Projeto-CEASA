{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto CEASA\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Facilitar o acesso aos dados do Centrais de Abastecimento do Estado do Rio de Janeiro S.A (CEASA-RJ) por meio de um processo automatizado de coleta, transformação e análise de dados.\n",
    "\n",
    "! [diagrama de fluxo do projeto: coleta, transformação, armazenamento e visualização](./img/Diagram.png)\n",
    "\n",
    "## Motivação\n",
    "\n",
    "A necessidade de disponibilizar informações do CEASA-RJ de maneira mais acessível, transformando dados presentes em PDFs em formatos utilizáveis para análise e insights mais rápidos.\n",
    "\n",
    "## Etapas do Projeto\n",
    "1. Coleta de Dados\n",
    "2. Transformação de Dados\n",
    "3. Armazenamento\n",
    "4. Análise e Visualização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. configurações inicia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .venv/bin/activate\n",
    "# %pip install -r minimal.requirements.txt # type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # carregando variaveis do embiente \n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv(\".env\")\n",
    "# ceasa_lista_pdf\n",
    "\n",
    "# variaeis de embiente\n",
    "\n",
    "URL_CEASA   = 'https://www.ceasa.rj.gov.br'\n",
    "URL_CEASA_cotacao   = URL_CEASA + '/Cota%C3%A7%C3%A3o'\n",
    "\n",
    "PASTA_PDFs  = \"./pdfs/\"\n",
    "PASTA_DADOS = \"./dados/\"\n",
    "\n",
    "ceasa_lista_pdf = PASTA_DADOS + \"ceasa_lista_pdf.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barra de progresso\n",
    "# def print_percent_done(index, total, progress_state=None, bar_len=15, title='Processando'):\n",
    "    # # font:https://stackoverflow.com/questions/6169217/replace-console-output-in-python\n",
    "    # '''\n",
    "    # index is expected to be 0 based index. \n",
    "    # 0 <= index < total\n",
    "    # '''\n",
    "    # percent_done = (index+1)/total*100\n",
    "    # percent_done = round(percent_done, 1)\n",
    "\n",
    "    # done = round(percent_done/(100/bar_len))\n",
    "    # togo = bar_len-done\n",
    "\n",
    "    # done_str = '█'*int(done)\n",
    "    # togo_str = '░'*int(togo)\n",
    "\n",
    "    \n",
    "    # if round(percent_done) != 100:\n",
    "    #     progress_state = f\"\\t {index}/{total}status: {progress_state}\" if progress_state else ''\n",
    "\n",
    "    #     print(f'⏳{title}: [{done_str}{togo_str}] {percent_done}% done.{progress_state}', end='\\r')\n",
    "\n",
    "    # else:\n",
    "    #     print('\\t 100% ✅')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Coletar Dados\n",
    "\n",
    "consiste nas etapas:\n",
    "1. Encontar as urls dos documentos pdf\n",
    "2. com a url do documento, extrair nome e data do documento\n",
    "3. Baixar e armazenar url, nome do documento e data em uma base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coleta linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re #regex\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "def foi_extraida(url):\n",
    "\t\"\"\"\n",
    "\tverifica se uma url ja está contida na base de dados.\n",
    "\t\"\"\"\n",
    "\treturn True if url in((df_ceasa_lista_pdf['URL'].eq(url))) else False\n",
    "\n",
    "def pegar_links(url):\n",
    "\treqs = requests.get(url)\n",
    "\tsoup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "\n",
    "\tfor link in soup.select('#main a'):\n",
    "\t\ta = str(link.get('href'))\n",
    "\t\tif \".pdf\" in a:\n",
    "\t\t\ta = URL_CEASA + a\n",
    "\t\t\tif a in urls_na_base:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\turls.append(a)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\t\tpegar_links(a)\n",
    "\n",
    "def extrair_dados_url(url):\n",
    "\t# pega nome do arquivo na URL\n",
    "\tnome_arquivo = url.split(\"/\")[-1]\n",
    "\tnome_arquivo = requests.utils.unquote(nome_arquivo) # type: ignore\n",
    "\n",
    "\t# padrao de dd mm yyyy para data\n",
    "\tmatches = re.findall(r'(\\d{2})\\s(\\d{2})\\s(\\d{4})', nome_arquivo)\n",
    "\tdata = dt.strptime(\"/\".join(matches[0]), \"%d/%m/%Y\")\n",
    "\n",
    "\treturn url,\"CEASA-RJ_\" + nome_arquivo.replace(\" \", \"_\") ,data.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "def pegar_pdf(url,nome_arquivo):\n",
    "\t# conferir se ja está na base local\n",
    "\tresponse = requests.get(url)\n",
    "\n",
    "\tif response.status_code == 200:\n",
    "\t\twith open(PASTA_PDFs+nome_arquivo, \"wb\") as f:\n",
    "\t\t\tf.write(response.content)\n",
    "\t\t\n",
    "\t\treturn True\n",
    "\n",
    "def atualizar_csv(url, nome_arquivo,data):\n",
    "\tdata_to_file = {\n",
    "\t\t'URL': [url],\n",
    "\t\t'NOME_ARQUIVO': [nome_arquivo],\n",
    "\t\t'DATA': [data]\n",
    "\t} \n",
    "\tdf = pd.DataFrame(data_to_file)\n",
    "\t# df.parquer(ceasa_lista_pdf, mode='a', index=False, header=False)\n",
    "\tdf.to_parquet(ceasa_lista_pdf, index=False, engine='fastparquet', append=True)\n",
    "\n",
    "\n",
    "def atualizar_base_pdf():\n",
    "\t# atualiza as urls de novos PDFf\n",
    "\tpegar_links(URL_CEASA_cotacao)\n",
    "\t\n",
    "\t# Coleta pdf, armazena o arquivo e atualiza a base \n",
    "\tfor index, url in enumerate(urls):\n",
    "\t\tprint(f\"{index} de {len(urls)}\",end=\"\\t\\t\\r\")\n",
    "\n",
    "\t\t# if foi_extraida:\n",
    "\t\t# \tcontinue\n",
    "\t\t\n",
    "\t\turl, nome_arquivo, data = extrair_dados_url(url)\n",
    "\t\tpegar_pdf(url,nome_arquivo)\n",
    "\t\tatualizar_csv(url,nome_arquivo,data)\n",
    "\t\n",
    "\n",
    "\n",
    "try:\n",
    "\tdf_ceasa_lista_pdf = pd.read_parquet(ceasa_lista_pdf)\n",
    "except:\n",
    "\tprint(f'criando arquivo: {ceasa_lista_pdf}')\n",
    "\n",
    "\tdf_tmp = pd.read_parquet(ceasa_lista_pdf, names = [\"URL\",\"NOME_ARQUIVO\",\"DATA\"])\n",
    "\tdf_tmp.to_parquet(ceasa_lista_pdf,index=False)\n",
    "\n",
    "df_ceasa_lista_pdf = pd.read_parquet(ceasa_lista_pdf)\n",
    "\n",
    "urls_na_base = df_ceasa_lista_pdf[\"URL\"].tolist()\n",
    "urls = []\n",
    "\n",
    "atualizar_base_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Transformando Dados\n",
    "\n",
    "A parte mais deliada é garantir a consistencia dos dados extraidos dos documentos pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compressão paralelizada (muito custoso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import concurrent.futures\n",
    "\n",
    "def compress_pdf(input_path, output_path):\n",
    "    with open(input_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        pdf_writer = PyPDF2.PdfWriter()\n",
    "\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page.compress_content_streams()\n",
    "            pdf_writer.add_page(page)\n",
    "\n",
    "        with open(output_path, 'wb') as output_file:\n",
    "            pdf_writer.write(output_file)\n",
    "\n",
    "def process_pdf(file_name):\n",
    "    print(f'arquivo atual: {file_name}')\n",
    "    compress_pdf(PASTA_PDFs + file_name, PASTA_PDFs + file_name)\n",
    "\n",
    "# Sua lista de arquivos\n",
    "files_to_process = df_extração[\"nome_arquivo\"].tolist()\n",
    "\n",
    "# Número máximo de threads (ajuste conforme necessário)\n",
    "max_threads = 4\n",
    "\n",
    "# Usando ThreadPoolExecutor para paralelizar\n",
    "with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "    futures = [executor.submit(process_pdf, file_name) for file_name in files_to_process]\n",
    "\n",
    "    # Esperar que todas as threads concluam\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        count += 1 # type: ignore\n",
    "        print_percent_done(count, len(files_to_process), future.result()) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo Tabelas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apartir de 01-03-2023 ouver mudança na formatação da tabela.\n",
    "trataremos as duas versões de PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "def extrair_tabela_pdf(pdf_path, pagina=0):\n",
    "    # Abre o PDF com pdfplumber\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Seleciona a página desejada\n",
    "        page = pdf.pages[pagina]\n",
    "        \n",
    "        # Extrai a tabela como uma lista de dicionários\n",
    "        table_data = page.extract_table()\n",
    "\n",
    "        # Converte a lista de dicionários para um DataFrame do pandas\n",
    "        df = pd.DataFrame(table_data)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fazendo tratamento na tabela 1 do pdf modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabeçalho\n",
    "header = ['PRODUTOS', 'TIPO', 'UNIDADE EMBALAGEM', 'VARIAÇÃO ULTIMOS 12 MESES', 'MIN', 'MODAL', 'MAX']\n",
    "\n",
    "#c Classes de alimentos\n",
    "classes = ['1. FRUTAS NACIONAIS',\n",
    "          '2. FRUTAS IMPORTADAS',\n",
    "          '3. HORTALIÇAS FRUTO',\n",
    "          '4. HORTALIÇAS FOLHA, FLOR E HASTE',\n",
    "          '5. HORTALIÇAS RAIZ, BULBO,TUBÉRCULO E RIZOMA',\n",
    "          '6. OVOS',\n",
    "          '7. PEIXE']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tramento PDF v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"#VALOR!\" at position 150",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32mlib.pyx:2368\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"#VALOR!\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/labs/Projeto ceasa/ETL_CEASA.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(list_df_tratamento_1_tbv2\u001b[39m.\u001b[39mcopy())\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tratamento_2_tbv2(df,pdf_v2,data,url) \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m df \u001b[39m=\u001b[39m pdf_v2_para_tabela(\u001b[39m'\u001b[39;49m\u001b[39mCEASA-RJ_Boletim_diário_de_preços__31_07_2023.pdf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m df\n",
      "\u001b[1;32m/root/labs/Projeto ceasa/ETL_CEASA.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39m# Concatenar os dataframes da lista\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(list_df_tratamento_1_tbv2\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tratamento_2_tbv2(df,pdf_v2,data,url)\n",
      "\u001b[1;32m/root/labs/Projeto ceasa/ETL_CEASA.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Remover o símbolo de percentagem\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Remover o símbolo de percentagem\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mto_numeric(df[\u001b[39m'\u001b[39;49m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m  \u001b[39m# Converter para float e dividir por 100\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mastype({\u001b[39m'\u001b[39m\u001b[39mVARIAÇÃO ULTIMOS 12 MESES\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Convert text to uppercase in columns: 'PRODUTOS', 'TIPO'\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/numeric.py:222\u001b[0m, in \u001b[0;36mto_numeric\u001b[0;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[1;32m    220\u001b[0m coerce_numeric \u001b[39m=\u001b[39m errors \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     values, new_mask \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmaybe_convert_numeric(  \u001b[39m# type: ignore[call-overload]  # noqa: E501\u001b[39;49;00m\n\u001b[1;32m    223\u001b[0m         values,\n\u001b[1;32m    224\u001b[0m         \u001b[39mset\u001b[39;49m(),\n\u001b[1;32m    225\u001b[0m         coerce_numeric\u001b[39m=\u001b[39;49mcoerce_numeric,\n\u001b[1;32m    226\u001b[0m         convert_to_masked_nullable\u001b[39m=\u001b[39;49mdtype_backend \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m lib\u001b[39m.\u001b[39;49mno_default\n\u001b[1;32m    227\u001b[0m         \u001b[39mor\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(values_dtype, StringDtype),\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    230\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32mlib.pyx:2410\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"#VALOR!\" at position 150"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# tratamendo geral pdf v2\n",
    "def tratamento_1_tbv2(df):\n",
    "    # define cabeçalhos\n",
    "    df.columns = header\n",
    "    \n",
    "    # Remove primeiras linhas\n",
    "    df = df.iloc[2: , :]\n",
    "    \n",
    "    # Adiciona colula classe\n",
    "    df.loc[:, 'CLASSE'] = ''\n",
    "\n",
    "    return df\n",
    "\n",
    "def tratamento_2_tbv2(df, arquivo='', data_pdf = '', url='' ):\n",
    "    # preencher class\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Colulas de arquivo e data\n",
    "    df['DATA'] = data_pdf\n",
    "    df['ARQUIVO'] = arquivo\n",
    "    df['URL'] = url\n",
    "    \n",
    "    # preenche classe\n",
    "    def preenche_classe(df):\n",
    "        df = df.reset_index(drop=True)    \n",
    "        \n",
    "        current_class = ''  # Para rastrear a classe atual\n",
    "        for index, row in df.iterrows():\n",
    "            if row['PRODUTOS'] in classes:\n",
    "                current_class = row['PRODUTOS']\n",
    "            df.at[index,'CLASSE'] = current_class\n",
    "    \n",
    "        df = df[df['PRODUTOS'] != df['CLASSE']]\n",
    "        df = df.reset_index(drop=True)    \n",
    "    \n",
    "        return df\n",
    "    \n",
    "    df = preenche_classe(df)\n",
    "\n",
    "    \n",
    "    # Replace all instances of \",\" with \".\" in columns: 'MIN', 'MODAL', 'MAX'\n",
    "    df['MIN'] = df['MIN'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "    df['MODAL'] = df['MODAL'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "    df['MAX'] = df['MAX'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "\n",
    "    # Replace all instances of \"\" with \"0\" in columns: 'MIN', 'MODAL', 'MAX'\n",
    "    df.loc[df['MIN'].str.lower() == \"\".lower(), 'MIN'] = \"0\"\n",
    "    df.loc[df['MODAL'].str.lower() == \"\".lower(), 'MODAL'] = \"0\"\n",
    "    df.loc[df['MAX'].str.lower() == \"\".lower(), 'MAX'] = \"0\"\n",
    "\n",
    "    # Função para limpar os valores da coluna\n",
    "    def limpar_valor(valor):\n",
    "        # Remover símbolos duplicados\n",
    "        valor = ''.join(ch for ch, _ in zip(valor, valor[1:] + '\\0') if ch != _)\n",
    "        # Remover espaços em branco duplicados\n",
    "        valor = ' '.join(valor.split())\n",
    "        return valor\n",
    "\n",
    "    # Aplicar a função de limpeza à coluna\n",
    "    df['MIN'] = df['MIN'].apply(limpar_valor)\n",
    "    df['MAX'] = df['MAX'].apply(limpar_valor)\n",
    "    df['MODAL'] = df['MODAL'].apply(limpar_valor)\n",
    "\n",
    "    # df = df.applymap(lambda x: limpar_valor(x))  # Remove espaços no início e no final\n",
    "    df = df.map(lambda x: str(x).strip())  # Remove espaços no início e no final\n",
    "    df = df.map(lambda x: ' '.join(str(x).split()))  # Remove espaços duplicados\n",
    "\n",
    "\n",
    "    # Change column type to float32 for columns: 'MIN', 'MODAL', 'MAX'\n",
    "    df = df.astype({'MIN': 'float32', 'MODAL': 'float32', 'MAX': 'float32'})\n",
    "\n",
    "    # Replace all instances of \"S/C\" with \"0\" in column: 'VARIAÇÃO ULTIMOS 12 MESES'\n",
    "    df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace(\"S/C\", \"0\", case=False, regex=False)\n",
    "\n",
    "    # Replace all instances of \"\" with \"\" in column: 'VARIAÇÃO ULTIMOS 12 MESES'\n",
    "    df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace(\"\", \"\", case=False, regex=False)\n",
    "\n",
    "    df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace('%', '')  # Remover o símbolo de percentagem\n",
    "    df['VARIAÇÃO ULTIMOS 12 MESES'] = df['VARIAÇÃO ULTIMOS 12 MESES'].str.replace(',', '.')  # Remover o símbolo de percentagem\n",
    "    df['VARIAÇÃO ULTIMOS 12 MESES'] = pd.to_numeric(df['VARIAÇÃO ULTIMOS 12 MESES']) / 100  # Converter para float e dividir por 100\n",
    "    df = df.astype({'VARIAÇÃO ULTIMOS 12 MESES': 'float32'})\n",
    "\n",
    "\n",
    "    # Convert text to uppercase in columns: 'PRODUTOS', 'TIPO'\n",
    "    df['PRODUTOS'] = df['PRODUTOS'].str.upper()\n",
    "    df['TIPO'] = df['TIPO'].str.upper()\n",
    "\n",
    "    # Change column type to string for columns: 'PRODUTOS', 'TIPO'\n",
    "    df = df.astype({'PRODUTOS': 'string', 'TIPO': 'string','UNIDADE EMBALAGEM': 'string'})\n",
    "\n",
    "    # Convert text to uppercase in column: 'UNIDADE EMBALAGEM'\n",
    "    df['UNIDADE EMBALAGEM'] = df['UNIDADE EMBALAGEM'].str.upper()\n",
    "\n",
    "    # Replace all instances of \",\" with \".\" in column: 'UNIDADE EMBALAGEM'\n",
    "    df['UNIDADE EMBALAGEM'] = df['UNIDADE EMBALAGEM'].str.replace(\",\", \".\", case=False, regex=False)\n",
    "\n",
    "    # Round columns 'VARIAÇÃO ULTIMOS 12 MESES', 'MIN' and 2 other columns (Number of decimals: 4)\n",
    "    df = df.round({'VARIAÇÃO ULTIMOS 12 MESES': 4, 'MIN': 2, 'MODAL': 2, 'MAX': 2})\n",
    "\n",
    "    # Change column type to datetime64[ns] for column: 'DATA\n",
    "    df = df.astype({'CLASSE': 'string', 'ARQUIVO': 'string', 'URL': 'string','DATA': 'datetime64[ns]'})\n",
    "\n",
    "    # Sort by column: 'DATA' (descending)\n",
    "    df = df.sort_values(['DATA'], ascending=[False])\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pdf_v2_para_tabela(pdf_v2, data=None, url=''):        \n",
    "    # Compreensão de lista para criar a lista de dataframes\n",
    "    list_df_tratamento_1_tbv2 = [ tratamento_1_tbv2( extrair_tabela_pdf(PASTA_PDFs + pdf_v2, i)) for i in range(5) ]\n",
    "\n",
    "    # Concatenar os dataframes da lista\n",
    "    df = pd.concat(list_df_tratamento_1_tbv2.copy())\n",
    "    return tratamento_2_tbv2(df,pdf_v2,data,url) # type: ignore\n",
    "\n",
    "\n",
    "df = pdf_v2_para_tabela('CEASA-RJ_Boletim_diário_de_preços__31_07_2023.pdf')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erroe 103 2023-07-31 00:00:00\t\t\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/labs/Projeto ceasa/ETL_CEASA.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m         \u001b[39m# df_dados_pdfs_v2 = pd.read_parquet(df_dados_pdfs_v2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39m# # df_dados_pdfs_v2 = df_dados_pdfs_v2[\"URL\"].drop_duplicates()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m         \u001b[39m# print(df_pdfs_v2.shape[0],\" / \",df_dados_pdfs_v2.shape[0])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m         \u001b[39m# # df_pdfs_v2 = df_pdfs_v2[~df_pdfs_v2['URL'].isin(df_dados_pdfs_v2)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df_pdfs_v2\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m df \u001b[39m=\u001b[39m extrair_tb_pdf_V2()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# df.to_csv(PASTA_DADOS + 'dados_pdfs_v2.csv',index=False, mode='a')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     df\u001b[39m.\u001b[39mto_parquet(PASTA_DADOS \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdados_pdfs_v2.parquet\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,engine\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m'\u001b[39m, append\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/root/labs/Projeto ceasa/ETL_CEASA.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39merro\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(lista_pdf_v2)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/root/labs/Projeto%20ceasa/ETL_CEASA.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    378\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    381\u001b[0m     objs,\n\u001b[1;32m    382\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    383\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    384\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    385\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    386\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    387\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    388\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    389\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    390\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 443\u001b[0m objs, keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    445\u001b[0m \u001b[39m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    446\u001b[0m ndims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    502\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "import os\n",
    "\n",
    "\n",
    "# Suprime os avisos SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "\n",
    "def extrair_tb_pdf_V2():\n",
    "    # comparar com\n",
    "    df_pdfs_v2 = filtar_para_novos_pdf()\n",
    "    # df_pdfs_v2 = pd.read_parquet(ceasa_lista_pdf)\n",
    "    \n",
    "    df_pdfs_v2 = df_pdfs_v2[df_pdfs_v2['DATA']  >= '2023-03-01']\n",
    "    df_pdfs_v2 = df_pdfs_v2[:]\n",
    "    \n",
    "    len = df_pdfs_v2.shape[0]\n",
    "    lista_pdf_v2 = []\n",
    "    try:\n",
    "        for index, row in df_pdfs_v2.iterrows():\n",
    "            arquivo = row['NOME_ARQUIVO']\n",
    "\n",
    "            data = row['DATA']\n",
    "            url = row['URL']\n",
    "            \n",
    "            print(f\"{index} de {len}\", data, end=\"\\t\\t\\r\")\n",
    "\n",
    "            df_tmp = pdf_v2_para_tabela(\n",
    "                arquivo, data, url)\n",
    "\n",
    "            lista_pdf_v2.append(df_tmp)\n",
    "    except:\n",
    "        print(\"erro\")\n",
    "\n",
    "    finally:\n",
    "        df = pd.concat(lista_pdf_v2)\n",
    "        return df\n",
    "\n",
    "def filtar_para_novos_pdf():\n",
    "\n",
    "    df_pdfs_v2 = pd.read_parquet(ceasa_lista_pdf)\n",
    "    df_dados_pdfs_v2 = PASTA_DADOS + \"dados_pdfs_v2.parquet\"\n",
    "    \n",
    "    if os.path.exists(df_dados_pdfs_v2):\n",
    "\n",
    "        df_pdfs_v2 = pd.read_parquet(ceasa_lista_pdf)\n",
    "\n",
    "        df_dados_pdfs_v2 = pd.read_parquet(PASTA_DADOS + \"dados_pdfs_v2.parquet\")\n",
    "        df_dados_pdfs_v2 = df_dados_pdfs_v2.groupby(['DATA']).agg(C=('DATA', 'first')).reset_index()\n",
    "\n",
    "        df_pdfs_v2 = df_pdfs_v2[~df_pdfs_v2['DATA'].isin(df_dados_pdfs_v2['DATA'])]\n",
    "\n",
    "        # df_dados_pdfs_v2 = pd.read_parquet(df_dados_pdfs_v2)\n",
    "        # # df_dados_pdfs_v2 = df_dados_pdfs_v2[\"URL\"].drop_duplicates()\n",
    "        # print(df_pdfs_v2.shape[0],\" / \",df_dados_pdfs_v2.shape[0])\n",
    "\n",
    "        # df_pdfs_v2=pd.merge(df_pdfs_v2,df_dados_pdfs_v2,on='DATA', how='left')\n",
    "\n",
    "        # # df_pdfs_v2 = df_pdfs_v2[~df_pdfs_v2['URL'].isin(df_dados_pdfs_v2)]\n",
    "\n",
    "    return df_pdfs_v2\n",
    "\n",
    "\n",
    "df = extrair_tb_pdf_V2()\n",
    "\n",
    "try:\n",
    "    # df.to_csv(PASTA_DADOS + 'dados_pdfs_v2.csv',index=False, mode='a')\n",
    "    df.to_parquet(PASTA_DADOS + 'dados_pdfs_v2.parquet',index=False,engine='fastparquet', append=True)\n",
    "except:\n",
    "    # df.to_csv(PASTA_DADOS + 'dados_pdfs_v2.csv',index=False)\n",
    "    df.to_parquet(PASTA_DADOS + 'dados_pdfs_v2.parquet',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
